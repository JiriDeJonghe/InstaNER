# Do you really need an LLM in every part of your processes?

For a long time, training AI models was limited to data scientists. Training good models from scratch has alwasy been a costly endeavour. Creating a model is a multi-stage process where expertise in each step of the process is required to create an optimal model. Generally speaking, the process can be divided in the following parts:
1. **Data collection**: AI models are trained on data and therefore, data collection is a logical first step of the process. However, data collection is costly: high quality data is difficult to find. The collected data should have the following properties:
- **Representative**: the collected data must reflect real-word scenarios that the model will encounter. For example, a tornado damage prediction model trained on data from regions with lots of tornados (e.g., the Caribbean) will not be able to be used for areas with different environments (e.g., Europe).
- **Varied**: the dataset should contain diverse scenarios such that the model is able to generalize to new, unseen data. For instance, a model used to detect fraudulent transfers might not be able to pick up new fraud patterns that were not present in the training data.
2. **Data wrangling**: oftentimes the raw data you collected in the previous stage is not yet usable to train a model on. The formatting might be wrong, the data might contain parts you need to remove...This is where data wrangling comes in: it is the process of preparing the data and making it available in a user-friendly way to train the model.
- **Cleaning**: removing or correcting inaccurate, incomplete, or irrelevant data. For example, handling missing values, removing duplicates, or correcting typos.
- **Transformation**: Converting data into a suitable format or structure for analysis.
3. **Training the model**: finally, we're able to train the model. Once again, there a lot of variables to take into account:
- **Model Architecture**: different problems require different architectures. A CNN might be required for image related tasks, while boosting models might better suit for numerical data.
- **Hyperparameter Tuning**: training a model can be done in many different ways within the same architecture. For example, choosing the split of your training-test data and learning rate.
4. **Evaluating the model**: after the model has been trained, it is crucial to evaluate the model to get an understanding of how the model performs. Does it achieve the expected results or should we iterate our previous steps to increase the performance of the model?

While this is still all true, the coming of LLMs shook up the traditional approaches: many use-cases for which you used to have to train a separate model can now easily be done by an LLM using zero-shot prompting. Just think of use-case such as intent detection, where you decide whether a sentence is positive, neutral, or negative. Due to the inherent language capabilities of LLMs there is no need to train these types of models anymore, just plug-in your LLM, send an API request, parse the response and get on with your day. In this post I make a comparison between the two approaches (e.g, training a model using the above listed steps or simply using an LLM) and suggest a third approach, based on synthetic data and model distilation.

LLMs are convenient. With ever-increasing capabilities as we've seen the past two years, writing a simple prompt can already get you a long way, and in the cases where this doesn't suffice, you might introduce few-shot prompting, chain-of-thought prompting or other more advanced prompting techniques. Furthermore, with the commodization we've been seeing (with DeepSeek for example) it is cheaper than ever before to make use of these incredibly powerful models. Recently, the AI agent trend has taken off, and they are already capable of doing incredible things. However, when designing your system, I urge to you re-evaluate: do you really need an LLM in every part of your processes?

## Limitations of LLMs
As most of us, I remember being absolutely amazed the first time I used ChatGPT. It really felt like magic, and more importantly, it felt like it was able to answer any question you threw at it. After using it more and more, the limitations became more apparent: hallucinations, difficulties with instruction following, inconsistencies, prompt sensitivity...And suddenly it all felt a little less magical. And this is not just me, a common sentiment you will find is people wondering if "the LLM has gotten dumber". People love to speculate about the reason: did the LLM provider update the weights? Or did they change the system prompt perhaps? Maybe they are taking shortcuts and actually using a smaller model instead. All of these I've read multiple times over online. I believe none of these are true. Humans simply ran into the limitations of LLMs.

Unfortunately, LLMs have still plenty of problems to solve:
- **Hallucinations**: this is by far, up to this point, one of the (if not the) most common problem people encounter when using LLMs. The over-confidence of the LLM makes it even more difficult to detect where the LLM started to hallucinate. When integrating LLMs in processes, hallucinations can, without the proper safeguards, produce a funny result at best, and crash the entire process at worst.
- **Inconsistencies in instruction following**: this problem especially can be very annoying to deal with when integrating an LLM in your process. You might ask the exact same LLM the exact same question, with the exact same prompt and get good responses 99 out of a 100 times. But this one time the LLM might decide to not strictly follow the given instructions and return its answer to your question in a somewhat different way, resulting in the subsequent steps failing, due to the expetency of a certain format of the LLM.
- **Prompt Sensitivity**: you might've crafted the most masterful prompt the world has ever seen, pushing the LLM to its absolute limits and you're able to extract the best answer from this LLM. Now suppose you need to add one teeny tiny change to the prompt. Suddenly, your results are significantly worse and the LLM is making mistakes that it wasn't making before that are completely unrelated to the change you made.
- **Incontrollability**: as a user of an LLM (unless you deploy it locally yourself) you're relient on a third party. Suppose now that the people from before were right, and that OpenAI or any other third party LLM provider did in fact change their model without notifying. The model you rely on might from one day to another be completely unusable. It would not be the first time tech companies enshittified their product after obtaining a large market share.

The above issues can actually all be led back to a single part: **explainability**. The LLM is just a black box that is trained on a gigantuous amount of data and that amazingly obtained the capabilities that it has. And, although asking the LLM explicitely to provide it's reasoning can help, there is no strict way of knowing how the LLM really thinks. The chain of thought that can be seen using reasoning models is simply a text layer on top of their actual thinking, trying to make it align with the expected results of humans. Indeed, DeepSeek R1-Zero switches languages in it's reasoning. Is this because it is hallucinating? I believe it is related to the fact that Chinese is a more information dense language, able to represent more information using fewer amount of characters. This hypothesis is somewhat supported by this research by Meta. They trained to models together in the art of barthering and they developed their own, more compact, information dense language. The reason why this doesn't happen with other reasoning models is because they are trained using some shape of RLHF, where they are rewarded for producing reasoning that aligns with human understanding.

## Alternative approach
So it seems, we are at an impasse: on the one hand, creating a model from scratch costs a lot of money and time, and, on the other hand, there are still substantial issues with LLMs that cannot be ignored and can potentially be dealbreakers for operation crucial applications. What if we combine the two approaches and try to get the best parts of both, while limiting the downsides to a minimum? Note the following: the issues of our first approach mainly lie in getting the model operational, once the model has been trained we can move it to production and use it for inference at a low cost. Admittedly, there are still plenty of issues that might pop during this stis process, but handling these issues is something we're already quite good at! Contrarily, the issues of our second approach lie during usage time: we don't create the model and have very little control of its behaviours besides prompting and/or fine-tunin. What if we could switch now these two around: using a self trained model for inference while using an LLM during the model creation stage? This way we can leverage the immense capabilities of LLMs while still having the maturity of tradition MLOps. For this, we use **synthetic data**.

Synthetic data creation is the process of generating data that is not directly taken from human sources. Instead, the LLM is very capable to generate this dataset, one of the side-effects of pre-training the model on a giant amount of data. This would indeed make sense: if a model is able to classify a sentence based on sentiment, then it might also be able to generate sentences with a specific sentiment.

Next up, we can use this synthetic data to train a our narrow model on. Note that the goal is not to distill the LLM into a smaller model that would inherit all the capabilities of the large LLM. No, oftentimes we only really need a fraction of the capabilities of the LLM to get a result. Where the LLM is considered to be *general* AI, meaning that is applicable on a wide variety of tasks, the model we're aiming to create is *narrow* AI, which should be able to perform one task and one task only. Therefore, we only need to extract a tiny part of the knowledge of the LLM.

You can look at it like this: an LLM is like using a flamethrower to light some candles. You might light the candles, you might set your house on fire. Maybe it is better to stick to simple matches.

## Synthetic Data Generation
In itself, synthetic data generation is not a new concept. Already a lot of research has been conducted in finding ways to artifically generate new data. Up to this point, this was usually done using extrapolation methods that require an amount of datapoints such that is able to generalize.

Now, remember the previously specified requirements for *high quality* data:
1. **Representative**: where before we might've used a relatively small amount of data to extrapolate to create new synthetic data, with LLMs this is not strictly required. Simply inputing the prompt "Create me 10 sentences and classify them as positive, negative, or neutral" will already do the trick. In some cases this might suffice, however, when applying it to a specific use-case where you encounter sentences in a more niche domain you might want to provide more information. For example, imagine you get an email where the person signs it with "thanks in advance for your most speedy delivery". While this sentence might seem neutral or even slightly positive in a general context, in business context this comes across as more passive-agressive and thus should be classified as negative. In cases such as these it is better to steer the model more using e.g., few-shot prompting
2. **Variety**: there are plenty of ways to ensure that the dataset is varied; upping the temperate, using different prompts, using different examples in few-shot prompting...In general, the more varied your prompts for data creation are, the more varied your results are. Of course, keep in mind that the created data should still be representative. Nevertheless, this is still a remaining difficulty and it is still very much possible that your data is not yet varied enough, especially when your application should cover lots of different use-cases. There is plenty of research going on into more complex methods for ensuring variety in the generation of your synthetic data.


## InstaNER
To show this approach, I introduce InstaNER, an easy-to-use framework that allows you to train a Named Entity Recognition (NER) model. In NER you're trying to identify all the named entities in asentence. For example, take the sentence "John was walking in Chicago". There are two entities that can be found in this sentence, namely John and Chicago. This doesn't seem like a very difficult task, and to be honest, it isn't. The issue however is that collecting a Named Entity Recognition dataset has historically been rather expensive. Although great datasets are freely available, there are usually a few limitations that strongly limit the capabilities:
1. *Entities are ambiguous*: you would think that it is quite easy to identify whether something is an entity or not, but suppose now the following sentence: "John Smith was walking in Chicago". Should "John Smith" be considered a single entity with the label "NAME"? What now if we want to detect first- and last name separately? In this case we would have to find a new dataset or start labeling ourselves. Many such cases can be found, for example, you also want to extract data surrounding adress, house number etc. Some datasets might make this distinction, others won't. Some datasets won't even regard an address as an entity, other datasets do.
2. *Low resource languages*: English is the lingua franca and the most used language when creating datasets. However, many of the required applications still require interactions in other languages. A wide array of resources in this language is not a given, and having these resources in English is a luxury which many do simply not have.

In conclusion, finding a dataset that actually perfectly fits your needs is difficult. I remember having to label data for multiple days straight simply because the data we were working on was unconvential and written in Dutch and French. I can confidently say that it was not an enjoyable experience, and, in some way, InstaNER is all born out of spite.

InstaNER sets up an entire pipeline that goes through the following steps:
1. We create synthetic data using an LLM for training a NER model. The user provides the entities that they want the model to recognize and can provide example sentences that can serve as a starting point for the LLM to base the generated data on. Furthermore, the user provides the language in which it wants the created dataset to be, leveraging the power of pre-training and LLMs to generate data even in low-resource languages.
2. After the synthetic data has been created, you train the NER model. Creating the NER model is achieved by fine-tuning a BERT model. A different model is used depending on the language provided. Some languages already have a BERT model that has been pre-trained on this language, if not, we use a multi-lingual BERT model.
3. Once the model has been trained, we of course evaluate the performance the model on our test-set, a part of the generated dataset on which the model has not been trained.
4. Finally, a qualitative check can be performed by automatically loading the trained model for inference.

All the used data, hyperparameters, and fine-grained evaluation results are saved together in a directory, allowing for easy reproduceability.

There are two ways to use the tool: using the CLI to start the `main` function with arguments or using the Model Creation Agent (ironic isn't it?) allowing non-technical people to easily use the tool as well.

## Closing thoughts
I hope that with this blogpost and tool showcase it makes you reconsider using an LLM as a silver bullet and shoehorning it in every possible step of your application. LLMs are an amazing tool and are being integrated all around us, but let's not forget that these models are not perfect and that there are always tradeoffs when using this technology. Suppose that an LLM has a chance of 99% to do what you expect it to do. If you use this same LLM ten times in the same pipeline, the pipeline will fail in 10% of the cases.
