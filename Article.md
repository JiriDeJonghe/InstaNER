# Do you really need an LLM in every part of your processes?

For a long time, training AI models was limited to data scientists. Training good models from scratch has alwasy been a costly endeavour. Creating a model is a multi-stage process where expertise in each step of the process is required to create an optimal model. Generally speaking, the process can be divided in the following parts:
1. **Data collection**: AI models are trained on data and therefore, data collection is a logical first step of the process. However, data collection is costly: high quality data is difficult to find. The collected data should have the following properties:
- **Representative**: the collected data must reflect real-word scenarios that the model will encounter. For example, a tornado damage prediction model trained on data from regions with lots of tornados (e.g., the Caribbean) will not be able to be used for areas with different environments (e.g., Europe).
- **Varied**: the dataset should contain diverse scenarios such that the model is able to generalize to new, unseen data. For instance, a model used to detect fraudulent transfers might not be able to pick up new fraud patterns that were not present in the training data.
2. **Data wrangling**: oftentimes the raw data you collected in the previous stage is not yet usable to train a model on. The formatting might be wrong, the data might contain parts you need to remove...This is where data wrangling comes in: it is the process of preparing the data and making it available in a user-friendly way to train the model.
- **Cleaning**: removing or correcting inaccurate, incomplete, or irrelevant data. For example, handling missing values, removing duplicates, or correcting typos.
- **Transformation**: Converting data into a suitable format or structure for analysis.
3. **Training the model**: finally, we're able to train the model. Once again, there a lot of variables to take into account:
- **Model Architecture**: different problems require different architectures. A CNN might be required for image related tasks, while boosting models might better suit for numerical data.
- **Hyperparameter Tuning**: training a model can be done in many different ways within the same architecture. For example, choosing the split of your training-test data and learning rate.
4. **Evaluating the model**: after the model has been trained, it is crucial to evaluate the model to get an understanding of how the model performs. Does it achieve the expected results or should we iterate our previous steps to increase the performance of the model?

While this is still all true, the coming of LLMs shook up the traditional approaches: many use-cases for which you used to have to train a separate model can now easily be done by an LLM using zero-shot prompting. Just think of use-case such as intent detection, where you decide whether a sentence is positive, neutral, or negative. Due to the inherent language capabilities of LLMs there is no need to train these types of models anymore, just plug-in your LLM, send an API request, parse the response and get on with your day. In this post I make a comparison between the two approaches (e.g, training a model using the above listed steps or simply using an LLM) and suggest a third approach, based on synthetic data and model distilation.

LLMs are convenient. With ever-increasing capabilities as we've seen the past two years, writing a simple prompt can already get you a long way, and in the cases where this doesn't suffice, you might introduce few-shot prompting, chain-of-thought prompting or other more advanced prompting techniques. Furthermore, with the commodization we've been seeing (with DeepSeek for example) it is cheaper than ever before to make use of these incredibly powerful models. Recently, the AI agent trend has taken off, and they are already capable of doing incredible things. However, when designing your system, I urge to you re-evaluate: do you really need an LLM in every part of your processes?

## Limitations of LLMs
As most of us, I remember being absolutely amazed the first time I used ChatGPT. It really felt like magic, and more importantly, it felt like it was able to answer any question you threw at it. After using it more and more, the limitations became more apparent: hallucinations, difficulties with instruction following, inconsistencies, prompt sensitivity...And suddenly it all felt a little less magical. And this is not just me, a common sentiment you will find is people wondering if "the LLM has gotten dumber". People love to speculate about the reason: did the LLM provider update the weights? Or did they change the system prompt perhaps? Maybe they are taking shortcuts and actually using a smaller model instead. All of these I've read multiple times over online. I believe none of these are true. Humans simply ran into the limitations of LLMs.

Unfortunately, LLMs have still plenty of problems to solve:
- **Hallucinations**: this is by far, up to this point, one of the (if not the) most common problem people encounter when using LLMs. The over-confidence of the LLM makes it even more difficult to detect where the LLM started to hallucinate. When integrating LLMs in processes, hallucinations can, without the proper safeguards, produce a funny result at best, and crash the entire process at worst.
- **Inconsistencies in instruction following**: this problem especially can be very annoying to deal with when integrating an LLM in your process. You might ask the exact same LLM the exact same question, with the exact same prompt and get good responses 99 out of a 100 times. But this one time the LLM might decide to not strictly follow the given instructions and return its answer to your question in a somewhat different way, resulting in the subsequent steps failing, due to the expetency of a certain format of the LLM.
- **Prompt Sensitivity**: you might've crafted the most masterful prompt the world has ever seen, pushing the LLM to its absolute limits and you're able to extract the best answer from this LLM. Now suppose you need to add one teeny tiny change to the prompt. Suddenly, your results are significantly worse and the LLM is making mistakes that it wasn't making before that are completely unrelated to the change you made.
- **Incontrollability**: as a user of an LLM (unless you deploy it locally yourself) you're relient on a third party. Suppose now that the people from before were right, and that OpenAI or any other third party LLM provider did in fact change their model without notifying. The model you rely on might from one day to another be completely unusable. It would not be the first time tech companies enshittified their product after obtaining a large market share.

The above issues can actually all be led back to a single part: **explainability**. The LLM is just a black box that is trained on a gigantuous amount of data and that amazingly obtained the capabilities that it has. And, although asking the LLM explicitely to provide it's reasoning can help, there is no strict way of knowing how the LLM really thinks. The chain of thought that can be seen using reasoning models is simply a text layer on top of their actual thinking, trying to make it align with the expected results of humans. Indeed, DeepSeek R1-Zero switches languages in it's reasoning. Is this because it is hallucinating? I believe it is related to the fact that Chinese is a more information dense language, able to represent more information using fewer amount of characters. This hypothesis is somewhat supported by this research by Meta. They trained to models together in the art of barthering and they developed their own, more compact, information dense language. The reason why this doesn't happen with other reasoning models is because they are trained using some shape of RLHF, where they are rewarded for producing reasoning that aligns with human understanding.

## Alternative approach
So it seems, we are at an impasse: on the one hand, creating a model from scratch costs a lot of money and time, on the other hand, there are still substantial issues with LLMs that cannot be ignored and can be dealbreakers for operation crucial applications. What now if we combine the two approaches and try to get the best parts of both, while not having the worst. Indeed, note the following: the issues of our first approach mainly lie in getting the model operational. Once it is operational you still have to do some monitoring, but this is something we're already quite good at! On the other, the issues of our second approach lie during usage time (since we don't create the model ourselves). What if we could switch now these two around: using a local model for inference while using an LLM during the model creation stage? This way we can leverage the immense capabilities of LLMs while still having the maturity of tradition MLOps. For this, we use **synthetic data**.

Synthetic data is data that has been created by an AI model, based on previously seen data. In other words, we fabricate data using an LLM. This would indeed make sense: if a model is able to classify a sentence based on sentiment, then it might also be able to generate sentences with a specific sentiment. 

## InstaNER

## Closing thoughts
